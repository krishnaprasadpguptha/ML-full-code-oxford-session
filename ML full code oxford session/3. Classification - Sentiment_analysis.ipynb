{"cells":[{"cell_type":"markdown","source":["# Problem statement: Classification model to analyze Amazon product reviews\n","\n","The objective is to create a classification model that will analyze Amazon product reviews to classify sentiments as positive or negative. Here's a breakdown of the steps involved in this workflow:\n","\n","- Step 1: Load the Dataset\n","- Step 2: Data Pre-processing\n","- Step 3: Feature Selection\n","- Step 4: Model Selection\n","- Step 5: Training the Model\n","- Step 6: Model Evaluation\n","- Step 7: Hyperparameter Tuning\n","- Step 8: Cross Validation\n","\n","The notebook contains 7 exercises in total:\n","\n","* [Exercise 1](#ex_1)\n","* [Exercise 2](#ex_2)\n","* [Exercise 3](#ex_3)\n","* [Exercise 4](#ex_4)\n","* [Exercise 5](#ex_5)\n","* [Exercise 6](#ex_6)\n","* [Exercise 7](#ex_7)"],"metadata":{"id":"IRGQ0MBqOnEL"}},{"cell_type":"markdown","metadata":{"id":"uH6KQFuh1XHu"},"source":["## Step 1: Load the dataset\n","First, let's load the dataset from Google Drive. You need to upload the dataset and then read the CSV file into a pandas DataFrame."]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"id":"MGvvWInefqU-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hfYy3kh11GCA"},"outputs":[],"source":["# Import necessary libraries\n","import pandas as pd\n","\n","# Load the dataset into a DataFrame\n","df = pd.read_csv('amazon-product-review-data.csv')\n","\n","# Display the first few rows to check if the data is loaded correctly\n","df.head()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"p8E6a6h_2LIC"},"source":["## Step 2: Data Pre-processing\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t_WMCjp72Z3g"},"outputs":[],"source":["# Import necessary libraries for data pre-processing\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Remove any rows with missing values\n","df.dropna(inplace=True)\n","\n","# Encode the 'sentiments' column (positive/negative) to numerical values (0/1)\n","le = LabelEncoder()\n","df['sentiments'] = le.fit_transform(df['sentiments'])\n","\n","# Text data preprocessing using TF-IDF vectorization\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n","X = tfidf_vectorizer.fit_transform(df['review_body']).toarray()\n","y = df['sentiments'].values\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Display the shapes of the resulting data\n","print(\"X_train shape:\", X_train.shape)\n","print(\"X_test shape:\", X_test.shape)\n","print(\"y_train shape:\", y_train.shape)\n","print(\"y_test shape:\", y_test.shape)\n"]},{"cell_type":"markdown","source":["<a name=\"ex_1\"></a>\n","## Exercise 1\n","\n","- Use the train_test_split function and change the test_size to 0.3\n","\n","This way the training set (X and y) should be 70% and the testing set(X and y) should be 30%"],"metadata":{"id":"eSAPAkPnTXrR"}},{"cell_type":"code","source":["#Write your code here"],"metadata":{"id":"-UWf6agSRon9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I9q_p7vr27mV"},"source":["## Step 3: Feature Selection\n","\n","In this step, we'll perform feature selection to reduce the dimensionality of the TF-IDF vectorized data and potentially improve the model's performance. We'll use feature selection techniques like chi-squared (chi2) or mutual information to select the most important features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8usH1IZP2-HS"},"outputs":[],"source":["from sklearn.feature_selection import SelectKBest, chi2\n","\n","# Apply feature selection using chi-squared (chi2) test\n","# You can adjust the number of features (k) as needed\n","k = 1000\n","selector = SelectKBest(chi2, k=k)\n","X_train_selected = selector.fit_transform(X_train, y_train)\n","X_test_selected = selector.transform(X_test)\n","\n","# Display the shapes of the selected feature sets\n","print(\"X_train_selected shape:\", X_train_selected.shape)\n","print(\"X_test_selected shape:\", X_test_selected.shape)"]},{"cell_type":"markdown","source":["<a name=\"ex_2\"></a>\n","## Exercise 2\n","\n","- Compare the X_train_selected shape and X_test_selected shape with the new test_size=0.3"],"metadata":{"id":"SU3KOde8Te0k"}},{"cell_type":"code","source":["#Write your code here"],"metadata":{"id":"tcoq_fCqDpaE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"slW7wA0L3Q3R"},"source":["We have successfully performed feature selection, reducing the dimensionality of the data while retaining the most important features.\n","\n","\n","## Step 4: Model Selection\n","For sentiment analysis, you can use various machine learning algorithms like Logistic Regression, Naive Bayes, Support Vector Machines, or even deep learning models like LSTM or BERT. Since you're a beginner, let's start with a simple model like Logistic Regression."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Olqw9Aah3Wx6"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","\n","# Initialize the Logistic Regression model\n","model = LogisticRegression(random_state=42)\n"]},{"cell_type":"markdown","source":["<a name=\"ex_3\"></a>\n","## Exercise 3\n","\n","What does the random_state (parameter of the LogisticRegression) represent?"],"metadata":{"id":"-eaR-mvhTpwp"}},{"cell_type":"markdown","source":["**Answer**: Write your answer here"],"metadata":{"id":"7oSfXOImTxrq"}},{"cell_type":"markdown","metadata":{"id":"7KTqnTiB3leR"},"source":["## Step 5: Training the Model\n","\n","Now that we have initialized our Logistic Regression model, it's time to train it on the selected features from the training dataset.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFqbF79I3sFJ"},"outputs":[],"source":["\n","# Train the Logistic Regression model on the selected features\n","model.fit(X_train_selected, y_train)\n","\n","# We can now proceed to Step 7: Model Evaluation"]},{"cell_type":"markdown","metadata":{"id":"PtEOm5Y630p5"},"source":["## Step 6: Model Evaluation\n","\n","In this step, we'll evaluate the performance of the trained Logistic Regression model using the testing data.\n","\n","- We import necessary metrics from `sklearn.metrics` such as `accuracy_score`, `classification_report`, and `confusion_matrix`.\n","- We use the trained model to predict sentiment labels (`y_pred`) for the test data (`X_test_selected`).\n","- We calculate the accuracy of the model by comparing the predicted labels to the true labels.\n","- We display a classification report that includes precision, recall, F1-score, and support for both positive and negative sentiment classes.\n","- We display a confusion matrix to visualize the true positive, true negative, false positive, and false negative predictions.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4eNG5rY5323C"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","# Predict sentiment labels for the test data\n","y_pred = model.predict(X_test_selected)\n","\n","# Calculate the accuracy of the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)\n","\n","# Display a classification report\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred))\n","\n","# Display a confusion matrix\n","print(\"\\nConfusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred))"]},{"cell_type":"markdown","source":["<a name=\"ex_4\"></a>\n","## Exercise 4\n","\n","- Compare the Results with the new data split with the results of the actual split."],"metadata":{"id":"VUCBrmL4UASm"}},{"cell_type":"code","source":["# Write your code here"],"metadata":{"id":"pY-p78AhRVCi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"ex_5\"></a>\n","## Exercise 5\n","\n","Do different training and testing sizes impact the model's learning and response to new data?"],"metadata":{"id":"JOQTfrhqUKiw"}},{"cell_type":"markdown","source":["**Answer**: Write your answer here"],"metadata":{"id":"W15I941KUQWx"}},{"cell_type":"markdown","metadata":{"id":"7L7dQj714Ckw"},"source":["## Step 7: Hyperparameter Tuning\n","\n","In this step, we'll perform hyperparameter tuning to optimize the Logistic Regression model's performance. We can search for the best hyperparameters using techniques like Grid Search or Random Search.\n","\n","- We import `GridSearchCV` from `sklearn.model_selection`.\n","- We define a grid of hyperparameters to search, including 'C' (regularization parameter) and 'max_iter' (maximum iterations).\n","- We initialize Grid Search with cross-validation (5-fold) to find the best hyperparameters.\n","- The best hyperparameters are extracted using `grid_search.best_params_`.\n","- We fit the tuned model with the best hyperparameters to the training data.\n","- Finally, we evaluate the tuned model's accuracy on the test data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OX7ebRMa4GBT"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","# Define hyperparameters to search\n","param_grid = {\n","    'C': [0.1, 1, 10, 100],  # Regularization parameters\n","    'max_iter': [100, 200, 300]  # Maximum number of iterations\n","}\n","\n","# Initialize Grid Search with cross-validation (5-fold)\n","grid_search = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=5, verbose=1, n_jobs=-1)\n","\n","# Fit the Grid Search to the data\n","grid_search.fit(X_train_selected, y_train)\n","\n","# Get the best hyperparameters\n","best_params = grid_search.best_params_\n","print(\"Best Hyperparameters:\", best_params)\n","\n","# Evaluate the model with the best hyperparameters\n","best_model = grid_search.best_estimator_\n","y_pred_tuned = best_model.predict(X_test_selected)\n","\n","# Calculate the accuracy of the tuned model\n","accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n","print(\"Tuned Model Accuracy:\", accuracy_tuned)"]},{"cell_type":"markdown","source":["<a name=\"ex_6\"></a>\n","## Exercise 6\n","\n","- What is GridSearchCV used for?\n","- What are hyperparameters?\n","- Does the model give better results after hyperparameters ?"],"metadata":{"id":"FaJSToyvUbBp"}},{"cell_type":"markdown","source":["**Answer**: Write your answer here"],"metadata":{"id":"zhZdGMyeUk4J"}},{"cell_type":"markdown","metadata":{"id":"5Axwiyok4PkA"},"source":["It appears that the hyperparameter tuning did not significantly improve the model's accuracy in this case. The accuracy remains at 0.86.\n","\n","## Step 8: Cross Validation\n","\n","We'll use cross-validation to estimate how well the model will perform on unseen data and check if the model's performance is consistent across different folds of the data.\n","\n","- We import `cross_val_score` from `sklearn.model_selection`.\n","- We perform 5-fold cross-validation on the tuned model (`best_model`) using the training data (`X_train_selected` and `y_train`).\n","- We calculate the mean cross-validation accuracy to get a more robust estimate of the model's performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ezr_YRmn4VkI"},"outputs":[],"source":["from sklearn.model_selection import cross_val_score\n","\n","# Perform 5-fold cross-validation on the tuned model\n","cv_scores = cross_val_score(best_model, X_train_selected, y_train, cv=5)\n","\n","# Calculate and display the mean cross-validation accuracy\n","mean_cv_accuracy = np.mean(cv_scores)\n","print(\"Mean Cross-Validation Accuracy:\", mean_cv_accuracy)"]},{"cell_type":"markdown","source":["<a name=\"ex_7\"></a>\n","## Exercise 7\n","\n","- What is Cross Validation used for?\n","- Compare the new Validation score (with the new training and testing size)\n","- What do you conclude ?"],"metadata":{"id":"SaZiGRevUrOG"}},{"cell_type":"markdown","source":["**Answer**: Write your answer here"],"metadata":{"id":"9zifBq1xVJ5b"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}