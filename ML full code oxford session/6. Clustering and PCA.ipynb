{"cells":[{"cell_type":"markdown","metadata":{"id":"-LVulCVswZF4"},"source":["# Problem statement\n","\n","The primary goal is to develop a Product Categorization model leveraging clustering techniques. By analyzing the E-Commerce dataset provided by The UCI Machine Learning Repository, this model aims to automatically group products into meaningful categories based on their features, such as product description, price, customer reviews, and more.\n","\n","The dataset contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retailer.\n","\n","The dataset contains the following columns:\n","- InvoiceNo (integer)\n","- StockCode (string)\n","- Description (string)\n","- Quantity (integer)\n","- InvoiceDate (date)\n","- UnitPrice (decimal)\n","- CustomerID (integer)\n","- Country (string)\n","\n","The notebook contains 4 exercises in total:\n","\n","* [Exercise 1](#ex_1)\n","* [Exercise 2](#ex_2)\n","* [Exercise 3](#ex_3)\n","* [Exercise 4](#ex_4)"]},{"cell_type":"markdown","metadata":{"id":"kc7e0bv5wZF5"},"source":["## Steps to follow\n","\n","- **Load the Dataset**: Use Pandas to read the CSV file.\n","- **Preprocess the Data**: Clean and preprocess the Description text data.\n","- **Feature Extraction**: Use TF-IDF to convert text descriptions into a vectorized format.\n","- **Clustering**: Apply the K-Means algorithm to cluster products into categories based on their descriptions.\n","- **Dimensionality Reduction for Visualization**: Use PCA to reduce the dimensions of the TF-IDF vectors for visualization.\n","- **Visualize Clusters**: Create a 3D scatter plot of the clusters."]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"id":"JE6urnxtKSvA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","from sklearn.decomposition import PCA\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","# Ensure necessary NLTK resources are downloaded\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('punkt_tab')\n","\n","# Step 2: Load the Dataset\n","# Update this path to where you have stored the dataset\n","df = pd.read_csv('clustering-data.csv', encoding='latin1')\n","\n","# Step 3: Preprocess the Data\n","def preprocess_text(text):\n","    # Tokenize, remove stopwords, and keep only alphabetic words\n","    tokens = word_tokenize(text.lower())\n","    tokens = [word for word in tokens if word.isalpha() and word not in stopwords.words('english')]\n","    return ' '.join(tokens)\n","\n","# Applying the preprocessing function to the Description column\n","df['Processed_Description'] = df['Description'].dropna().apply(preprocess_text)\n","\n","# Step 4: Feature Extraction\n","tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n","tfidf_matrix = tfidf_vectorizer.fit_transform(df['Processed_Description'].dropna())\n","\n","# Step 5: Clustering\n","num_clusters = 5  # You might want to adjust this based on experimentation\n","kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n","clusters = kmeans.fit_predict(tfidf_matrix)\n","\n","# Step 6: Dimensionality Reduction for Visualization\n","pca = PCA(n_components=3)\n","reduced_features = pca.fit_transform(tfidf_matrix.toarray())\n","\n","# Step 7: Visualize Clusters\n","fig = plt.figure(figsize=(10, 7))\n","ax = fig.add_subplot(111, projection='3d')\n","scatter = ax.scatter(reduced_features[:,0], reduced_features[:,1], reduced_features[:,2],\n","                     c=clusters, cmap='viridis')\n","ax.set_title('Product Clusters')\n","plt.show()\n"],"metadata":{"id":"FXmzDri4KXgN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" Visualise most frequent words occurring in product descriptions assigned to clusters."],"metadata":{"id":"2R9L-4Ss6GYu"}},{"cell_type":"code","source":["from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","\n","def generate_word_clouds(n_clusters, cluster_assignments, documents):\n","    \"\"\"\n","    Generates and displays word clouds for each cluster.\n","\n","    Parameters:\n","    - n_clusters (int): Number of clusters.\n","    - cluster_assignments (array-like): Cluster assignment for each document.\n","    - documents (array-like): Preprocessed text documents.\n","    \"\"\"\n","    # Iterate through each cluster\n","    for cluster in range(n_clusters):\n","        # Filter documents belonging to the current cluster\n","        cluster_documents = [doc for doc, assignment in zip(documents, cluster_assignments) if assignment == cluster]\n","\n","        # Join documents into a single string\n","        text = \" \".join(cluster_documents)\n","\n","        # Generate a word cloud\n","        wordcloud = WordCloud(width = 800, height = 800,\n","                              background_color ='white',\n","                              stopwords = stopwords.words('english'),\n","                              min_font_size = 10).generate(text)\n","\n","        # Plot the word cloud\n","        plt.figure(figsize = (8, 8), facecolor = None)\n","        plt.imshow(wordcloud)\n","        plt.axis(\"off\")\n","        plt.tight_layout(pad = 0)\n","        plt.title(f'Word Cloud for Cluster {cluster}')\n","        plt.show()\n","\n","# Example usage\n","generate_word_clouds(num_clusters, clusters, df['Processed_Description'].dropna().tolist())\n"],"metadata":{"id":"dmBQlUbdPA2b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"ex_1\"></a>\n","## Exercise 1\n","\n","Use the describe() function to generate descriptive statistics of the dataset.\n","\n","This function provides a concise summary of the columns, including count, mean, standard deviation, min, max, and quartile values.\n","\n","**Questions**:\n","- What insights can you derive from the output of the describe() method regarding the scale of the data and its distribution?\n","- How might these statistics influence your preprocessing decisions?"],"metadata":{"id":"vEIaq0OgEL2V"}},{"cell_type":"markdown","source":["**Answer**: Write your answer here"],"metadata":{"id":"9sR0coI1ZNaH"}},{"cell_type":"markdown","source":["<a name=\"ex_2\"></a>\n","## Exercise 2\n","\n","- In the code block above, what does apply function do?\n","\n","- In the code block above, why is the number of clusters = 5?\n","\n","- In the code block above, change the random_state=42 to values such as 80, 100, or 120 and explain how does it impact the results of the kmeans model?\n"],"metadata":{"id":"laoTaGXujWz1"}},{"cell_type":"markdown","source":["**Answer**: Write your code here"],"metadata":{"id":"UTNMz5fgcieD"}},{"cell_type":"markdown","source":["<a name=\"ex_3\"></a>\n","## Exercise 3\n","\n","- When is PCA needed ?\n","- How is the number of Dimensions calculated ?"],"metadata":{"id":"WKSsP7yLcGXL"}},{"cell_type":"markdown","source":["**Answer**: Write your answer here"],"metadata":{"id":"Ov771oORc2sb"}},{"cell_type":"markdown","source":["<a name=\"ex_4\"></a>\n","## Exercise 4\n","\n","- Compare the results from applying Kmeans directly to the processed dataset with results after applying PCA.\n","- What do you conclude ?"],"metadata":{"id":"z4sSQSKKcJ8F"}},{"cell_type":"markdown","source":["**Answer**: Write your answer here"],"metadata":{"id":"uqDxEOeic6IN"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}