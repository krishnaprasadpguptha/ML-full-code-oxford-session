{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Problem statement: Create a classification model for the Fashion MNIST\n","\n","The objective is to create a classification model for the Fashion MNIST dataset using a Multi-Layer Perceptron (MLP).\n","\n","We'll follow these steps:\n","\n","### 1. Data Preprocessing\n","- **Loading the Data**: Fashion MNIST is a dataset of Zalando's article images, with 60,000 training samples and 10,000 test samples. Each sample is a 28x28 grayscale image, associated with a label from 10 classes.\n","- **Normalization**: We normalize the pixel values (ranging from 0 to 255) to a scale of 0 to 1. This improves the training efficiency.\n","- **Reshaping for MLP**: Since we are using an MLP, we need to reshape the 28x28 images into a flat array of 784 pixels.\n","\n","### 2. Building the MLP Model\n","- **Dense Layers**: These are fully connected neural layers. The first layer needs to know the input shape (784 in this case).\n","- **Activation Functions**: 'ReLU' is used for non-linear transformations. The final layer uses 'softmax' for a probability distribution over 10 classes.\n","\n","### 3. Compiling the Model\n","- **Optimizer**: 'Adam' is a popular choice for its adaptive learning rate properties.\n","- **Loss Function**: 'sparse_categorical_crossentropy' is suitable for multi-class classification problems.\n","- **Metrics**: We'll use 'accuracy' to understand the performance.\n","\n","### 4. Training the Model\n","- We train the model using the `fit` method, specifying epochs and batch size.\n","\n","### 5. Evaluating the Model\n","- The `evaluate` method is used to test the model on the test set.\n","\n","The notebook contains one exercise in total:\n","\n","* [Exercise 1](#ex_1)"],"metadata":{"id":"7-1RI2LOhzW2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yKlxud86hcIh"},"outputs":[],"source":["# Import necessary libraries\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten\n","from tensorflow.keras.datasets import fashion_mnist\n","from tensorflow.keras.utils import to_categorical\n","\n","# Load the dataset\n","(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n","\n","# Normalize the images to [0, 1]\n","train_images = train_images / 255.0\n","test_images = test_images / 255.0\n","\n","# Reshape data for MLP input\n","train_images = train_images.reshape((-1, 28*28))\n","test_images = test_images.reshape((-1, 28*28))\n","\n","# Build the MLP model\n","model = Sequential()\n","model.add(Dense(128, activation='relu', input_shape=(784,)))\n","model.add(Dense(10, activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(train_images, train_labels, epochs=10, batch_size=64)\n","\n","# Evaluate the model\n","test_loss, test_acc = model.evaluate(test_images, test_labels)\n","\n","print('Test accuracy:', test_acc)"]},{"cell_type":"markdown","source":["To improve the model's accuracy on the Fashion MNIST dataset, we can experiment with various techniques. Here are some strategies:\n","\n","1. **Increase Model Complexity**: Add more layers or increase the number of neurons in each layer to capture more complex patterns in the data.\n","\n","2. **Regularization**: Implement dropout or L1/L2 regularization to reduce overfitting.\n","\n","3. **Advanced Optimizers**: Experiment with different optimizers like SGD or RMSprop.\n","\n","4. **Learning Rate Scheduling**: Adjust the learning rate during training.\n","\n","5. **Data Augmentation**: Although not typical for MLPs, slight modifications to the input data can make the model more robust.\n","\n","6. **Early Stopping**: Stop training when the validation accuracy stops improving.\n","\n","7. **Hyperparameter Tuning**: Experiment with different activation functions, batch sizes, and epochs.\n","\n","8. **Batch Normalization**: This can help in faster convergence and overall performance improvement.\n","\n","Let's modify the previous code to incorporate some of these strategies."],"metadata":{"id":"yL1TJaEzjvVF"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Dropout, BatchNormalization\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Modified MLP model\n","model = Sequential()\n","model.add(Dense(256, activation='relu', input_shape=(784,)))\n","model.add(BatchNormalization())  # Batch normalization layer\n","model.add(Dropout(0.5))         # Dropout layer\n","model.add(Dense(128, activation='relu'))\n","model.add(BatchNormalization())  # Another batch normalization layer\n","model.add(Dropout(0.5))         # Another dropout layer\n","model.add(Dense(10, activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Early stopping callback\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n","\n","# Train the model with validation split\n","model.fit(train_images, train_labels, epochs=50, batch_size=64,\n","          validation_split=0.2, callbacks=[early_stopping])\n","\n","# Evaluate the model\n","test_loss, test_acc = model.evaluate(test_images, test_labels)\n","\n","print('Test accuracy:', test_acc)"],"metadata":{"id":"GkitFh5aij7B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The test accuracy decreased slightly in this case. This outcome highlights an important aspect of machine learning: improvements in model architecture don't always lead to better performance, and sometimes simpler models can outperform more complex ones, especially on smaller datasets like Fashion MNIST.\n","\n","Here are a few additional steps you can take to try and improve the model's performance:\n","\n","1. **Adjust the Dropout Rate**: The dropout rate of 0.5 might be too high, causing the model to lose relevant information. Try reducing it to 0.3 or 0.2.\n","\n","2. **Fine-Tune the Model Complexity**: The addition of more neurons might have made the model too complex. Try reducing the number of neurons in the dense layers.\n","\n","3. **Experiment with Different Optimizers**: While Adam is a strong general-purpose optimizer, sometimes others like SGD (with a momentum) or RMSprop might yield better results for specific problems.\n","\n","4. **Modify the Learning Rate**: Adjusting the learning rate of the Adam optimizer could also lead to better results. A lower learning rate with more epochs can sometimes achieve better generalization.\n","\n","5. **Experiment with Batch Sizes**: Smaller or larger batch sizes can impact the model's ability to generalize and learn effectively.\n","\n","6. **Cross-Validation**: Instead of a single validation split, use k-fold cross-validation for a more robust estimate of model performance.\n","\n","Let's adjust the code with some of these suggestions."],"metadata":{"id":"lGXeENRJjzVx"}},{"cell_type":"code","source":["# Adjust the model architecture and training parameters\n","model = Sequential()\n","model.add(Dense(128, activation='relu', input_shape=(784,)))\n","model.add(Dropout(0.3))         # Reduced dropout rate\n","model.add(Dense(64, activation='relu'))\n","model.add(Dropout(0.3))         # Reduced dropout rate\n","model.add(Dense(10, activation='softmax'))\n","\n","# Compile the model with a modified optimizer\n","model.compile(optimizer='adam',  # You can experiment with learning rate here\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Train the model with a different batch size\n","model.fit(train_images, train_labels, epochs=50, batch_size=32,  # Smaller batch size\n","          validation_split=0.2, callbacks=[early_stopping])\n","\n","# Evaluate the model\n","test_loss, test_acc = model.evaluate(test_images, test_labels)\n","\n","print('Test accuracy:', test_acc)"],"metadata":{"id":"T9z3khhbjrVp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The test accuracy has improved to 0.8778, which is a positive outcome. This result indicates that the adjustments made to the model architecture and training parameters were beneficial.\n","\n","However, achieving higher accuracy on a dataset like Fashion MNIST can be challenging, especially with a simple model like a Multi-Layer Perceptron (MLP). To potentially achieve even better results, consider the following additional steps:\n","\n","1. **Feature Engineering**: Although this is more limited with image data and MLPs, ensuring the input data is as informative and clean as possible is crucial.\n","\n","2. **Ensemble Methods**: Combine predictions from several models to improve accuracy. For example, train multiple MLPs with different architectures and average their predictions.\n","\n","3. **Convolutional Neural Networks (CNNs)**: For image data, CNNs are generally more effective than MLPs. They can capture spatial hierarchies in the data better due to their convolutional layers.\n","\n","4. **Hyperparameter Optimization**: Use techniques like grid search or random search to systematically explore different hyperparameter combinations.\n","\n","5. **Advanced Regularization Techniques**: Experiment with other regularization methods like L1 regularization or different dropout configurations.\n","\n","Let's adjust the code with some of these suggestions."],"metadata":{"id":"rmWbPC5gmHZP"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","\n","# Reshape data for CNN input\n","train_images_cnn = train_images.reshape((-1, 28, 28, 1))\n","test_images_cnn = test_images.reshape((-1, 28, 28, 1))\n","\n","# Build a simple CNN model\n","cnn_model = Sequential()\n","cnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n","cnn_model.add(MaxPooling2D((2, 2)))\n","cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n","cnn_model.add(MaxPooling2D((2, 2)))\n","cnn_model.add(Flatten())\n","cnn_model.add(Dense(64, activation='relu'))\n","cnn_model.add(Dense(10, activation='softmax'))\n","\n","# Compile the model\n","cnn_model.compile(optimizer='adam',\n","                  loss='sparse_categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","# Train the model\n","cnn_model.fit(train_images_cnn, train_labels, epochs=10, batch_size=64,\n","              validation_split=0.2)\n","\n","# Evaluate the model\n","test_loss, test_acc = cnn_model.evaluate(test_images_cnn, test_labels)\n","\n","print('CNN Test accuracy:', test_acc)"],"metadata":{"id":"BKPmmHqImJEi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"ex_1\"></a>\n","## Exercise 1: Improve the accuracy of the MLP model\n","1. Try different architectures and hyperparameters.\n","2. Use regularization techniques like L1 or L2 regularization.\n","3. Use dropout to reduce overfitting.\n","\n","Referans link: https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/"],"metadata":{"id":"WACMOuHj3wp-"}}]}